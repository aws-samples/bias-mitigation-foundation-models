{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Part 3: Responsible answers with Retrieval Augmented Generation (RAG)\n",
    "\n",
    "> *This notebook should work well in the `Data Science 3.0` kernel on Amazon SageMaker Studio. It requires Python v3.10+*\n",
    "\n",
    "In this notebook we'll explore how Amazon Bedrock LLMs can be integrated with trusted text data sources to deliver more reliable answers, and show some ways you can **evaluate and measure** systems like these using [LlamaIndex](https://gpt-index.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Context: Understanding Retrieval-Augmented Generation\n",
    "\n",
    "Because Large Language Models are trained to generate likely-seeming responses to initial inputs, they're sometimes prone to [\"hallucinate\"](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)) answers that sound coherent and even confident, but are factually incorrect or even self-contradictory.\n",
    "\n",
    "When we prompt an LLM with a contextless question like *\"Who is the president of the US?\"*, we're asking it to recall whatever relevant facts it might've learned during training, and create an answer from scratch: Which makes incorrect \"hallucinations\" much more likely.\n",
    "\n",
    "When we prompt an LLM to respond to a prompt from which the answer is mostly self-evident, such hallucinations are rare because the model can directly reference the input data when forming a response. For example: *If five cats can catch five mice in five minutes, how long will it take one cat to catch one mouse?*\n",
    "\n",
    "...But we can't pack the prompt with **ALL** the information a bot might need to answer every possible question... So what's the solution?\n",
    "\n",
    "With the **Retrieval-Augmented Generation** pattern (as shown in the diagram below), we:\n",
    "\n",
    "- Use a **search engine** to **retrieve the most relevant** document snippets from a reference corpus, based on the user's question\n",
    "- Push the original question and those few snippets **together** into the LLM prompt\n",
    "- Ask the LLM to answer the question based on the provided sources, or else say it doesn't know\n",
    "\n",
    "Some solutions might add an additional LLM call to transform the user's raw question into an optimized search query, to improve retrieval performance.\n",
    "\n",
    "![](imgs/rag-flow-dark.png \"Flow diagram of a RAG system in which the user's question is used to search a corpus, and then those search results are fed together with the user question into a LLM to generate a response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Initial setup\n",
    "\n",
    "To start exploring RAG patterns with a practical example, we'll first install some libraries that might not be present in the default notebook kernel image:\n",
    "\n",
    "- Amazon Bedrock [became generally available](https://www.langchain.com/) in September 2023, so we need new-enough versions of the AWS Python SDKs `boto3` and `botocore` to be able to call the service\n",
    "- [LangChain](https://python.langchain.com/docs/get_started/introduction) is an open-source framework for orchestrating common LLM patterns, that we'll use to simplify the code examples instead of building from basic Bedrock SDK calls.\n",
    "- [LlamaIndex](https://gpt-index.readthedocs.io/en/stable/) is an open-source framework to help integrate LLMs with trusted data sources, and measure the performance of data-connected LLM use-cases\n",
    "- [pypdf](https://pypdf.readthedocs.io/en/stable/) will enable us to read PDF document text from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --quiet \\\n",
    "    \"boto3>=1.28.63,<2\" \\\n",
    "    \"botocore>=1.31.63,<2\" \\\n",
    "    langchain==0.0.337 \\\n",
    "    llama-index==0.9.4 \\\n",
    "    pypdf==3.17.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the installs done, we'll load some libraries and initial setup that'll be useful later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "import os  # For dealing with folder paths\n",
    "from urllib.request import urlretrieve  # For fetching data from the web\n",
    "from typing import Tuple, List  # Type annotations for easier debugging\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3  # AWS SDK for Python\n",
    "import nest_asyncio  # Needed for some asyncio-based libs to work in Jupyter notebooks\n",
    "import pandas as pd  # For processing and displaying tabular data (dataframes)\n",
    "\n",
    "nest_asyncio.apply()  # Enable asyncio-based libs to work properly in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Download and pre-process documents with Titan Text Embeddings and LlamaIndex\n",
    "\n",
    "The general RAG pattern can be implemented using pretty much any search engine, including fully-managed options like [Amazon Kendra](https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/) and [Amazon OpenSearch](https://aws.amazon.com/blogs/machine-learning/build-a-powerful-question-answering-bot-with-amazon-sagemaker-amazon-opensearch-service-streamlit-and-langchain/). Since response quality is strongly dependent on the search result quality, it's often better to use **semantic search** tools than traditional keyword-based engines.\n",
    "\n",
    "In this example notebook, we'll create an in-memory semantic search index using:\n",
    "\n",
    "- [Amazon Titan Embeddings](https://aws.amazon.com/about-aws/whats-new/2023/09/amazon-titan-embeddings-generally-available/) on Amazon Bedrock, as a model to convert text of documents and user queries into numerical \"embedding\" vectors.\n",
    "- LlamaIndex [VectorStoreIndex](https://gpt-index.readthedocs.io/en/stable/module_guides/indexing/vector_store_guide.html), to index the generated document vectors in-memory and retrieve the most similar documents for incoming queries/questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download sample documents\n",
    "\n",
    "In this example we'll use just a single document for our RAG corpus: Amazon's 2022 annual letter to shareholders. Since the document itself is quite long, it'll still end up being split into multiple separate entries in the search index.\n",
    "\n",
    "First, run the cell below to download the file(s) locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the local folder to use and the data files to fetch:\n",
    "DATA_ROOT = \"./data\"\n",
    "URL_FILENAME_MAP = {\n",
    "    \"https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf\": \\\n",
    "        \"2022-Shareholder-Letter.pdf\"\n",
    "}\n",
    "\n",
    "# Create the local folder and download the data files:\n",
    "urls = [k for k in URL_FILENAME_MAP.keys()]\n",
    "filenames = [URL_FILENAME_MAP[k] for k in urls]\n",
    "\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "for url, filename in URL_FILENAME_MAP.items():\n",
    "    urlretrieve(url, os.path.join(DATA_ROOT, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can initially read the PDF files using LlamaIndex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "docs = SimpleDirectoryReader(input_files=[\"data/2022-Shareholder-Letter.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and vectorize the documents\n",
    "\n",
    "Text vectorization models typically place an upper limit on the length of text they can process as a single item, and anyway we'll want each search result to be reasonably short - for embedding results in the answer generation LLM prompt later.\n",
    "\n",
    "Because of this, we'll need to **split** our source document(s) into shorter passages for indexing. LlamaIndex's [TokenTextSplitter](https://gpt-index.readthedocs.io/en/latest/api/llama_index.node_parser.TokenTextSplitter.html) provides a utility for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=512, chunk_overlap=102\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert each document chunk into a single vector, we'll use the Amazon Titan Embeddings model.\n",
    "\n",
    "LlamaIndex supports LangChain-based models via the `LangchainEmbedding` class, and LangChain supports Bedrock, so we can simply chain the two together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "# Text-to-vector model used to map document chunks or user queries to numeric vectors:\n",
    "embed_model = LangchainEmbedding(\n",
    "    BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the splitting and vectorization settings defined, we're ready to define and run LlamaIndex `IngestionPipeline` to ingest the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.ingestion import IngestionPipeline\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=[text_splitter, embed_model])\n",
    "doc_nodes = pipeline.run(documents=docs)\n",
    "\n",
    "print(f\"Ingested {len(doc_nodes)} chunks from {len(docs)} source docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create and test the query engine\n",
    "\n",
    "With the chunking and vectorization complete, we're ready to index the data into a queryable store.\n",
    "\n",
    "Since the end-to-end querying will also include *generating* the text answer from the retrieved documents, we'll need to define our **text generation model** configuration here too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock # required until llama_index offers direct Bedrock integration\n",
    "from llama_index import ServiceContext, set_global_service_context, VectorStoreIndex\n",
    "\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 10,\n",
    "    \"max_tokens_to_sample\": 3000,  # Max response length\n",
    "}\n",
    "\n",
    "# Text-to-text model used to formulate final answer from search results:\n",
    "llm = Bedrock(model_id=\"anthropic.claude-instant-v1\", model_kwargs=model_kwargs_claude)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    chunk_size=512,\n",
    ")\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "vector_index = VectorStoreIndex(\n",
    "    nodes=doc_nodes,\n",
    "    service_context=service_context,\n",
    ")\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=5,  # The top k=5 search results will be fed through to the LLM prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can store the created index to the local file system, in case you'd like to re-load it into memory in future rather than re-creating it from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"./indices\", exist_ok=True)\n",
    "vector_index.storage_context.persist(\"./indices/amazon-shareholder-letters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our end-to-end RAG-powered query engine with an example question, for which the answer should be present in the [source document](data/2022-Shareholder-Letter.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In-context question:\n",
    "query = \"What is Amazon's investment strategy in generative AI?\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you should see a correct and relevant response! But of course it can't answer ***every*** question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Out-of-context question:\n",
    "print(query_engine.query(\"What new features will AWS launch at Re:Invent 2040?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Challenging cross-context question:\n",
    "print(\n",
    "    query_engine.query(\n",
    "        \"How many months of RxPass membership could I buy for the same cost as the minimum \"\n",
    "        \"grocery order that'd qualify for free delivery?\"\n",
    "    )\n",
    ")\n",
    "# ($35 grocery threshold per page 2; RxPass = $5/mo per page 5; therefore the answer is 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Automated, end-to-end RAG pipeline evaluation with LlamaIndex evaluators\n",
    "\n",
    "As shown above, although RAG solutions are powerful they can still fail to answer questions for a variety of reasons, including:\n",
    "\n",
    "- The corpus could not contain any relevant documents to answer the user's question\n",
    "- The search engine could fail to return the correct/relevant snippets to build an answer\n",
    "- The LLM could fail to compose a useful and correct answer from the (correct) retrieved snippets\n",
    "\n",
    "Therefore to quantify the quality and robustness of the solution, we'll need to take a **data-driven** approach and will be interested in **multiple metrics**.\n",
    "\n",
    "Although **human evaluation** would provide a useful gold-standard, the effort required is not as scalable as we'd like for large datasets and frequent system updates. Instead, [LlamaIndex's evaluation module](https://gpt-index.readthedocs.io/en/latest/optimizing/evaluation/evaluation.html) provides automated tools that **use LLMs to judge** result quality.\n",
    "\n",
    "In the sections below, we'll show 4 automated evaluations available in the tool:\n",
    "\n",
    "1. **Faithfulness**: Whether the final response is in agreement with (doesn't contradict) the retrieved document snippets.\n",
    "2. **Relevancy**: Whether the response and retrieved content were relevant to the query.\n",
    "3. **Correctness**: Whether the generated answer is relevant and agreeing with a reference answer.\n",
    "4. **Guidelines**: Evaluating a system against customisable, user-specified guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Faithfulness to source documents\n",
    "\n",
    "The **Faithfulness** metric compares final generated response with the source document snippets that were retrieved from search, and is useful for checking if the generative model introduced any inconsistencies or **hallucinations**.\n",
    "\n",
    "![](imgs/rag-eval-flow-faithfulness.png \"Flow diagram: After retrieving relevant content & generating RAG response, responses are evaluated for how they match to the retrieved documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "# We'll be using our 'successful' question and response from earlier:\n",
    "print(query)\n",
    "print(response)\n",
    "print(\"----------------\")\n",
    "\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
    "eval_result = faithfulness_evaluator.evaluate_response(response=response)\n",
    "\n",
    "print(\"Did test pass:\", eval_result.passing)\n",
    "print(f\"Test feedback:\\n{eval_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Relevancy to the user question\n",
    "\n",
    "The **Relevancy** metric checks whether the response (and retrieved source documents) are actually relevant to the user's question. This is useful for measuring if the query was actually answered by the response.\n",
    "\n",
    "![](imgs/rag-eval-flow-relevancy.png \"Flow diagram: After retrieving relevant content & generating RAG response, responses are evaluated for how they match to the original query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import RelevancyEvaluator\n",
    "\n",
    "relevancy_evaluator = RelevancyEvaluator(service_context=service_context)\n",
    "eval_result = relevancy_evaluator.evaluate_response(query=query, response=response)\n",
    "\n",
    "print(\"Did test pass:\", eval_result.passing)\n",
    "print(f\"Test feedback:\\n{eval_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring divergence between faithfulness and relevancy\n",
    "\n",
    "For an example of how **relevancy** and **faithfulness** can differ, consider the below question that isn't answered by any content in the source corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Out-of-context question:\n",
    "ooc_query = \"What is the generative ai strategy for other cloud providers?\"\n",
    "ooc_response = query_engine.query(ooc_query)\n",
    "print(ooc_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the corpus did not contain information to answer the question, the generated response is still on-topic to what was originally asked... Therefore the relevancy test should **PASS**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate relevance of result to the original question:\n",
    "relevancy_evaluator = RelevancyEvaluator(service_context=service_context)\n",
    "eval_result = relevancy_evaluator.evaluate_response(query=ooc_query, response=ooc_response)\n",
    "\n",
    "print(\"Did relevancy test pass:\", eval_result.passing)\n",
    "print(f\"Test feedback:\\n{eval_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However in this case, the faithfulness test **FAILS** because of the divergence between the retrieved content and what was originally asked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate faithfulness of response to retrieved content:\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
    "eval_result = faithfulness_evaluator.evaluate_response(response=ooc_response)\n",
    "\n",
    "print(\"Did faithfulness test pass:\", eval_result.passing)\n",
    "print(f\"Test feedback:\\n{eval_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Correctness by matching to a reference answer\n",
    "\n",
    "The **Correctness** metric requires a correct reference answer to be provided for the question, and compares whether the generated response agrees with this target answer. It's useful for a 'ground truth' perspective in cases where questions have a single canonical answer and those target answers have been collected already.\n",
    "\n",
    "![](imgs/rag-eval-flow-correctness.png \"Flow diagram: After retrieving relevant content & generating RAG response, responses are evaluated against a reference answer\")\n",
    "\n",
    "Below we'll use a batch of questions to evaluate on all three evaluators discussed above - Faithfulness, Relevancy, and Correctness - to evaluate the overall performance of our RAG application.\n",
    "\n",
    "First, we'll define the dataset and the utility function to perform the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import CorrectnessEvaluator\n",
    "\n",
    "eval_question_answer_pair = [\n",
    "    (\"What is the Amazon policy for return to work after pandemic?\",\n",
    "        \"Amazon has asked corporate employees to come back to office at least three days a week beginning May 2022.\"),\n",
    "    (\"What changes did Amazon do to overcome the challenges related to increasing cost in Stores fulfillment network?\",\n",
    "        \"During the early pandemic, with physical stores shut, our consumer business grew extraordinarily, with revenue increasing from $245B in 2019 to $434B in 2022. This growth meant doubling our fulfillment center footprint built over 25 years and substantially accelerating a last-mile transportation network now the size of UPS in about two years - no easy feat thanks to hundreds of thousands of Amazonians. However, with that rate and scale of change, much optimization was needed to yield intended productivity; over recent months, we scrutinized every process path in fulfillment centers and transportation, redesigning many processes and mechanisms, resulting in steady gains and cost reductions the last few quarters. We also took this occasion to make larger structural changes setting us up for lower costs and faster speed for years, like reevaluating our US fulfillment network organization. Until recently, Amazon operated one national network distributing inventory from fulfillment centers nationwide, but as this expanded to hundreds more nodes, connecting them efficiently became more complex. Last year, we started re-architecting our inventory placement strategy by leveraging our larger footprint to move from a national to a regionalized network model with eight interconnected regions operating largely self-sufficiently while still shipping nationally when necessary. We also continue improving our algorithms to predict regional inventory needs and have completed this regional rollout with early results like shorter travel distances, lower costs, less environmental impact, and faster customer delivery. Overall, we remain confident about our plans to lower costs, reduce delivery times, and build a meaningfully larger retail business with healthy margins.\"),\n",
    "    (\"By what percentage did AWS revenue grow year-over-year in 2022?\",\n",
    "        \"AWS had a 29% year-over-year ('YoY') revenue in 2022 on $62B revenue base.\"),\n",
    "    (\"Approximately how many new features and services did AWS launch in 2022 according to the passage?\",\n",
    "        \"AWS launched over 3,300 new features and services in 2022\"),\n",
    "    (\"Compared to Graviton2 processors, what performance improvement did Graviton3 chips deliver according to the passage?\",\n",
    "        \"In 2022, AWS delivered their Graviton3 chips, providing 25% better performance than the Graviton2 processors.\"),\n",
    "    (\"Which was the first inference chip launched by AWS according to the passage?\",\n",
    "        \"AWS launched their first inference chips (“Inferentia”) in 2019, and they have saved companies like Amazon over a hundred million dollars in capital expense.\"),\n",
    "    (\"What kind of throughput and latency improvements does the new Inferentia2 chip offer compared to the original Inferentia chip?\",\n",
    "        \"Inferentia2 chip, launched by AWS, offers up to four times higher throughput and ten times lower latency than our first Inferentia processor. \"),\n",
    "    (\"According to the passage, what percentage of Amazon's unit sales in today comes from third-party sellers?\",\n",
    "        \"Today, Amazon sells nearly every physical and digital retail item, with a vibrant third-party seller ecosystem that accounts for 60% of their unit sales. \"),\n",
    "    (\"According to the context, in what year did Amazon's annual revenue increase from $245B to $434B?\",\n",
    "        \"Amazon's annual revenue increased from $245B in 2019 to $434B in 2022.\"),\n",
    "]\n",
    "\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
    "relevancy_evaluator = RelevancyEvaluator(service_context=service_context)\n",
    "correctness_evaluator = CorrectnessEvaluator(service_context=service_context)\n",
    "\n",
    "\n",
    "def run_evals(qa_pairs: List[Tuple[str, str]], query_engine):\n",
    "    \"\"\"Loop through a Q&A dataset to run a batch evaluation with LlamaIndex\"\"\"\n",
    "    results_list = []\n",
    "    for question, reference_answer in qa_pairs:\n",
    "        response = query_engine.query(question)\n",
    "        generated_answer = str(response)\n",
    "        correctness_results = correctness_evaluator.evaluate(\n",
    "            query=question,\n",
    "            response=generated_answer,\n",
    "            reference=reference_answer\n",
    "        )\n",
    "        faithfulness_results = faithfulness_evaluator.evaluate_response(response=response)\n",
    "        relevancy_results = relevancy_evaluator.evaluate_response(query=question, response=response)\n",
    "        cur_result_dict = {\n",
    "            \"query\": question,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"correctness\": correctness_results.passing,\n",
    "            \"correctness_feedback\": correctness_results.feedback,\n",
    "            \"correctness_score\": correctness_results.score,\n",
    "            \"faithfulness\": faithfulness_results.passing,\n",
    "            \"faithfulness_feedback\": faithfulness_results.feedback,\n",
    "            \"faithfulness_score\": faithfulness_results.score,\n",
    "            \"relevancy\": relevancy_results.passing,\n",
    "            \"relevancy_feedback\": relevancy_results.feedback,\n",
    "            \"relevancy_score\": relevancy_results.score\n",
    "        }\n",
    "        results_list.append(cur_result_dict)\n",
    "    evals_df = pd.DataFrame(results_list)\n",
    "    return evals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Then, we can run the evaluation on the dataset and visualize the results in a dataframe:\n",
    "\n",
    "> ⏰ **Note:** This batch evaluation will make several LLM calls for each Q&A pair in the dataset, so may take a couple of minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluation_results = run_evals(eval_question_answer_pair, query_engine)\n",
    "\n",
    "print(f\"\"\"Average correctness score: {evaluation_results.correctness.mean()}\n",
    "Average faithfulness score: {evaluation_results.faithfulness.mean()}\n",
    "Average relevancey score: {evaluation_results.relevancy.mean()}\"\"\")\n",
    "\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Guideline Evaluation of Prompt Completions: Using LLaMaIndex\n",
    "\n",
    "The **Guidelines** evaluator, rather than implementing a fixed metric, provides functionality for you to specify your own evaluation criteria in natural language. This is useful for implementing additional checks for the business' specific concerns.\n",
    "\n",
    "![](imgs/rag-eval-flow-guidelines.png \"Flow diagram: After retrieving relevant content & generating RAG response, responses are evaluated with respect to a set of guidelines defined in natural language.\")\n",
    "\n",
    "First, we create a `GuidelineEvaluator` for each guideline to be checked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.evaluation import GuidelineEvaluator\n",
    "\n",
    "GUIDELINES = [\n",
    "    \"The response should fully answer the query.\",\n",
    "    \"The response should avoid being vague or ambiguous.\",\n",
    "    \"The response should not use toxic or profane language.\",\n",
    "    \"The response should not be bias or discriminatory.\",\n",
    "    \"The response should be specific and use statistics or numbers when possible.\",\n",
    "]\n",
    "\n",
    "evaluators = [\n",
    "    GuidelineEvaluator(service_context=service_context, guidelines=guideline)\n",
    "    for guideline in GUIDELINES\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define which question/response pair we're going to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is Amazon's generative AI strategy?\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can loop through the guidelines to test and show the result for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for guideline, evaluator in zip(GUIDELINES, evaluators):\n",
    "    eval_result = evaluator.evaluate_response(\n",
    "        query=query,\n",
    "        response=response,\n",
    "    )\n",
    "    print(\"================\")\n",
    "    print(f\"Guideline: {guideline}\")\n",
    "    print(f\"Pass: {eval_result.passing}\")\n",
    "    print(f\"Feedback: {eval_result.feedback}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook we introduced how the **Retrieval-Augmented Generation** pattern can be applied to generate more reliable answers and reduce hallucinations, by grounding LLM responses in dynamically-retrieved data from trusted sources.\n",
    "\n",
    "As demonstrated through this series of notebooks, responsible and effective application of text generation models requires mitigating and monitoring several different risks: From toxicity and hallucination, to potential bias and hijacking. We've explored a range of tools available to tackle these concerns - including prompt engineering approaches, additional guardrail models, and retrieval-augmented generation.\n",
    "\n",
    "To ensure robustness, businesses will typically need to take a data-driven approach and evaluate solutions across this range of criteria. Although a level of human evaluation will be important to build confidence, **automated evaluation techniques** as shown here can help to scale this monitoring more effectively: Especially when experimenting with different models, prompt templates, and other configurations.\n",
    "\n",
    "In fact, LLMs can even be used to propose test cases as in the example below:\n",
    "\n",
    "```python\n",
    "# DO NOT RUN IN LAB SETTING\n",
    "# from llama_index.evaluation import DatasetGenerator\n",
    "#\n",
    "# data_generator = DatasetGenerator.from_documents(docs)\n",
    "# eval_questions = data_generator.generate_questions_from_nodes()\n",
    "# eval_questions\n",
    "```\n",
    "\n",
    "So although LLM use-cases raise new governance concerns, they can also provide new tools to help tackle them. By pro-actively managing risks and scaling tests through automation, businesses can build robust solutions and deploy them with confidence."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
